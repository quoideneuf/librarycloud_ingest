<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:context="http://www.springframework.org/schema/context"
    xsi:schemaLocation="
    http://www.springframework.org/schema/beans
    http://www.springframework.org/schema/beans/spring-beans.xsd
    http://camel.apache.org/schema/spring
    http://camel.apache.org/schema/spring/camel-spring.xsd
    http://www.springframework.org/schema/context
    http://www.springframework.org/schema/context/spring-context-3.0.xsd"
    >
    <!-- AWS Configuration -->
    <context:property-placeholder location="classpath:/aws.properties" ignore-unresolvable="true"/>
    <context:property-placeholder location="classpath:/librarycloud.env.properties" />
    <bean name="awsCredentials" class="com.amazonaws.auth.BasicAWSCredentials">
        <constructor-arg value="${access.key}"/>
        <constructor-arg value="${secret.key}"/>
    </bean>
    <bean name="sqsClient" class="com.amazonaws.services.sqs.AmazonSQSAsyncClient">
        <constructor-arg><ref bean="awsCredentials"/></constructor-arg>
    </bean>
    <bean name="snsClient" class="com.amazonaws.services.sns.AmazonSNSClient">
        <constructor-arg><ref bean="awsCredentials"/></constructor-arg>
    </bean>
    <bean name="s3Client" class="com.amazonaws.services.s3.AmazonS3Client">
        <constructor-arg><ref bean="awsCredentials"/></constructor-arg>
    </bean>
    
    <!-- Custom S3 marshaller to place body on S3 -->
    <bean id="cloudbody" class="edu.harvard.libcomm.pipeline.MessageBodyS3Marshaller">
        <constructor-arg><ref bean="awsCredentials"/></constructor-arg>
        <constructor-arg value="20000" /> <!-- If message size in bytes is greater than this, save body to S3 -->
        <constructor-arg value="${librarycloud.s3.cache_bucket}.${librarycloud.sqs.environment}" />
    </bean>

    <!-- Set a shorter timeout for shutdown, primarily for testing purposes. Default is 300. -->
    <bean id="shutdown" class="org.apache.camel.impl.DefaultShutdownStrategy">
        <property name="timeout" value="30"/>
    </bean>
    <!-- Configure display of trace log messages -->
    <bean id="traceFormatter" class="org.apache.camel.processor.interceptor.DefaultTraceFormatter">
        <property name="showBody" value="false"/>
        <property name="showHeaders" value="false"/>
    </bean>

    <!-- Definition of beans that handle processing of items in the pipeline -->
    <bean id="marcSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.marc.MarcSplitter">
                <constructor-arg value="500000" /> <!-- Approx. max size of messages, in bytes -->
            </bean>
        </property>
    </bean>	

	<bean id="deleteSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.delete.DeleteFileSplitter"/>
        </property>
    </bean>

    <bean id="updateSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.update.UpdateFileSplitter"/>
        </property>
    </bean>

    <bean id="eadSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.ead.EADSplitter"/>
        </property>
    </bean>
    <bean id="eadRawSplitter" class="edu.harvard.libcomm.pipeline.RawSplitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.ead.EADRawSplitter"/>
        </property>
    </bean>
    <bean id="viaSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.via.VIASplitter"/>
        </property>
    </bean>
    <bean id="viaRawSplitter" class="edu.harvard.libcomm.pipeline.RawSplitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.via.VIARawSplitter"/>
        </property>
    </bean>

    <bean id="modsSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.mods.MODSSplitter"/>
        </property>
    </bean>

    <bean id="modsMessageSplitter" class="edu.harvard.libcomm.pipeline.MessageSplitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.mods.MODSSplitter"/>
        </property>
    </bean>

    <bean id="modsAggregator" class="edu.harvard.libcomm.pipeline.MODSAggregatorStrategy"/>

    <bean id="modsEADRawAggregator" class="edu.harvard.libcomm.pipeline.MODSRawAggregatorStrategy">
        <property name="source" value="OASIS"/>
    </bean>

    <bean id="modsVIARawAggregator" class="edu.harvard.libcomm.pipeline.MODSRawAggregatorStrategy">
        <property name="source" value="VIA"/>
    </bean>

    <bean id="modsRawAggregator" class="edu.harvard.libcomm.pipeline.MODSRawAggregatorStrategy">
        <property name="source" value="ALEPH"/>
    </bean>

    <bean id="extractPayloadProcessor" class="edu.harvard.libcomm.pipeline.ExtractPayloadProcessor"/>

    <bean id="modsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.marc.ModsProcessor"/>
        </property>
    </bean>
    <bean id="filterUnchangedProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.FilterUnchangedProcessor"/>
        </property>
    </bean>
    <bean id="filterUnchangedMarcProcessor" class="edu.harvard.libcomm.pipeline.FilterUnchangedMarcProcessor" />
    <bean id="holdingsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.HoldingsProcessor"/>
        </property>
    </bean>

    <bean id="drsExtensionsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.DRSExtensionsProcessor"/>
        </property>
    </bean>

    <bean id="drsExtensionsUpdateProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.DRSExtensionsUpdateProcessor"/>
        </property>
    </bean>

    <bean id="drsExtensionsNewUpdateProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.DRSExtensionsNewUpdateProcessor"/>
        </property>
    </bean>

    <bean id="solrDrsExtensionsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrDrsExtensionsProcessor">
                <property name="commitWithinTime" value="30000"/>
            </bean>
        </property>
    </bean>

    <bean id="addMarcLocationProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.AddMarcLocationProcessor">
                <property name="marcBaseUrl" value="https://s3.amazonaws.com/${librarycloud.s3.marc_bucket}.${librarycloud.sqs.environment}/" />
            </bean>
        </property>
    </bean>
    <bean id="stackscoreProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.StackScoreProcessor"/>
        </property>
    </bean>
    <bean id="lccProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.LCCProcessor"/>
        </property>
    </bean>
    <bean id="collectionsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.CollectionsProcessor"/>
        </property>
    </bean>
    <bean id="collectionUpdateProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.CollectionUpdateProcessor"/>
        </property>
    </bean>
    <bean id="removeRestrictedProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.PublishProcessor"/>
        </property>
    </bean>
    <bean id="solrLoadProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrProcessor">
                <property name="commitWithinTime" value="30000"/>
            </bean>
        </property>
    </bean>
    <bean id="solrLoadProcessorImmediate" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrProcessor"/>
        </property>
    </bean>

    <bean id="solrHoldingsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrHoldingsProcessor">
                <property name="commitWithinTime" value="30000"/>
            </bean>
        </property>
    </bean>

    <bean id="solrDeleteProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrDeleteProcessor"/>
        </property>
    </bean>

    <bean id="prepareURI" class="edu.harvard.libcomm.pipeline.PrepareURIForHTTPEndpoint" />

    <camelContext id="sqsContext" xmlns="http://camel.apache.org/schema/spring" xmlns:marcxml="http://www.loc.gov/MARC21/slim" trace="true">
        <!-- Environment-specific properties -->
        <propertyPlaceholder id="librarycloud-properties" location="classpath:/librarycloud.env.properties" />
        <!-- Error handling behavior -->
        <errorHandler id="eh" redeliveryPolicyRef="myPolicy" type="DeadLetterChannel"
            deadLetterUri="direct:dead-letter"/>
        <redeliveryPolicyProfile id="myPolicy" maximumRedeliveries="0"/>
        <route id="dead-letter">
            <from uri="direct:dead-letter"/>
            <to uri="log:edu.harvard.libcomm.deadletterqueue?level=ERROR"/>
            <!-- Post to the AWS dead letter queue, and handle errors (e.g. file size exceeded) -->
            <doTry>
                <transform>
                    <simple>${exception}${exception.stacktrace}${in.body}</simple>
                </transform>
                <to uri="aws-sqs://{{librarycloud.sqs.environment}}-dead-letter?amazonSQSClient=#sqsClient"/>
                <doCatch>
                    <!-- If can't post the full message to the dead letter queue, post only the headers to the queue -->
                    <exception>java.lang.Exception</exception>
                    <to uri="log:edu.harvard.libcomm.deadletterqueue?level=ERROR&amp;showAll=true"/>
                    <transform>
                        <simple>Error placing failed message on dead letter queue. Headers for affected message: ${headers}. Exception: ${exception}${exception.stacktrace}</simple>
                    </transform>
                    <doTry>
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-dead-letter?amazonSQSClient=#sqsClient"/>
                        <doCatch>
                            <!-- If can't even post the headers to the dead letter queue, log error and be done -->
                            <exception>java.lang.Exception</exception>
                            <transform>
                                <simple>Error placing failed message headers on dead letter queue. Headers for affected message: ${headers} Exception: ${exception}${exception.stacktrace}</simple>
                            </transform>
                            <to uri="log:edu.harvard.libcomm.deadletterqueue?level=ERROR&amp;showAll=true"/>
                        </doCatch>
                    </doTry>
                </doCatch>
            </doTry>
        </route>

        <!-- Setup bucket used by cloudbody marshalling service if it does not already exist -->
        <route id="forceCreationOfS3Bucket" errorHandlerRef="eh">
            <from uri="direct:nothing" />
            <to uri="aws-s3://{{librarycloud.s3.cache_bucket}}.{{librarycloud.sqs.environment}}?amazonS3Client=#s3Client" />
        </route>


        <!-- ============================== -->
        <!-- Aleph HOLDINGS ingest -->
        <!-- ============================== -->

        <route id="holdings-ingest-split" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-holdings?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800" />
            <doTry>
                <split streaming="true" parallelProcessing="true">
                    <method bean="marcSplitter" method="split"/>
                    <marshal><custom ref="cloudbody"/></marshal>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-split-holdings?amazonSQSClient=#sqsClient" />
                    <!--<to uri="file://{{librarycloud.files.basepath}}/split-holdings?fileName=${header.CamelSplitIndex}" />-->
                </split>
            <doCatch>
                <exception>java.lang.Exception</exception>
                <to uri="direct:dead-letter" />
            </doCatch>
            </doTry>
        </route>

        <route id="marcholdings-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-split-holdings?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:split-holdings?size=20&amp;blockWhenFull=true"/>
        </route>

        <route id="marcholdingstosolr" errorHandlerRef="eh">
            <from uri="seda:split-holdings?concurrentConsumers=4" />
            <!--<from uri="aws-sqs://{{librarycloud.sqs.environment}}-split-holdings?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />-->
            <!--<from uri="file://{{librarycloud.files.basepath}}/split-holdings"/>-->
            <process ref="solrHoldingsProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>
            <!--<to uri="file://{{librarycloud.files.basepath}}/done"/>-->
        </route>

        <!-- ============================== -->
        <!-- Aleph ingest and normalization -->
        <!-- ============================== -->

        <!-- Take a command file, which references a MARC file. Download the MARC file and split it into a 
             series of messages containing MARCXML -->
        <route id="aleph-ingest-split" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-aleph?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800" />
            <!-- Try block required for errors thrown out of the splitter -->
            <doTry>
                <split streaming="true" parallelProcessing="true">
                    <method bean="marcSplitter" method="split"/>
                    <marshal><custom ref="cloudbody"/></marshal>
                    <!-- <to uri="file://{{librarycloud.files.basepath}}/cloudbody?fileName=${header.CamelSplitIndex}" /> -->
                    <!-- Route to two locations. One copy is transformed to MODS and sent through the rest of the pipeline. 
                         The other copy is used to upload raw MARCXML data to S3 -->
                    <multicast parallelProcessing="true">
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-normalize-marcxml?amazonSQSClient=#sqsClient" />
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-upload-marc?amazonSQSClient=#sqsClient" />
                    </multicast>
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
         </route>
         
        <!-- Transform MARCXML to MODS -->
        <route id="marctomods-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-normalize-marcxml?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:normalize-marcxml?size=20&amp;blockWhenFull=true"/>
        </route>               
        <route id="marctomods" errorHandlerRef="eh">
            <from uri="seda:normalize-marcxml?concurrentConsumers=4" />
            <process ref="modsProcessor"/>
            <process ref="addMarcLocationProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient" />
        </route>

        <!-- Extract the aleph ID from the MARCXML, and upload the record to S3, keyed by aleph ID-->
        <route id="uploadmarc-seda" errorHandlerRef="eh">
            <!-- Note the reduced messages per poll, so we don't end up getting messages timed out, since this takes a while -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-upload-marc?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=1&amp;visibilityTimeout=1800&amp;extendMessageVisibility=true" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:upload-marc?size=20&amp;blockWhenFull=true"/>
        </route>               
        <route id="upload-marc" errorHandlerRef="eh">
            <from uri="seda:upload-marc?concurrentConsumers=4" />
            <process ref="extractPayloadProcessor" />
            <split>
                <tokenize token="marcxml:record" inheritNamespaceTagName="marcxml:collection" xml="true" />
                <setHeader headerName="recordFileName">
                    <xpath resultType="java.lang.String">
                        /marcxml:record/marcxml:controlfield[@tag='001']
                    </xpath>
                </setHeader>                
                <!--<process ref="filterUnchangedMarcProcessor"/>-->
                <choice>
                    <when>
                        <simple>${in.body} != ""</simple>
                        <setHeader headerName="CamelAwsS3Key">
                            <simple>${header.recordFileName.substring(0,9)}</simple>
                        </setHeader>
                        <setHeader headerName="CamelAwsS3CannedAcl">
                            <simple>PublicRead</simple>
                        </setHeader>
                        <to uri="aws-s3://{{librarycloud.s3.marc_bucket}}.{{librarycloud.sqs.environment}}?amazonS3Client=#s3Client" />
                    </when>
                </choice>
            </split>
        </route>

        <!-- ============================== -->
        <!-- OASIS ingest and normalization -->
        <!-- ============================== -->
        <route id="oasis-ingest-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}/ingest-oasis" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-oasis?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800&amp;extendMessageVisibility=true" />
            <to uri="seda:oasis-ingest-seda?size=4&amp;blockWhenFull=true"/>
        </route>

        <route id="oasis-ingest" errorHandlerRef="eh">
            <from uri="seda:oasis-ingest-seda?concurrentConsumers=4" />
            <!-- Try block required for errors thrown out of the splitter -->
            <doTry>
                <split streaming="true" parallelProcessing="true">
                    <method bean="eadSplitter" method="split"/>
                    <aggregate strategyRef="modsAggregator" completionSize="100" completionInterval="10000">
                        <correlationExpression>
                            <simple>all</simple>
                        </correlationExpression>
                        <completionPredicate>
                            <simple>${header.messageLength} > 150000</simple>
                        </completionPredicate>  
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient" />
                        <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-start?fileName=${header.CamelSplitIndex}" /> -->
                    </aggregate>
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
        </route>

        <!-- VIA ingest and normalization -->
        <route id="via-ingest" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}/ingest-via" />             -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-via?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800&amp;extendMessageVisibility=true" />
            <split>
                <xtokenize mode="t">//filepath</xtokenize>
                <bean ref="prepareURI" method="prepare"/>
                <!-- Download the file identified in the message header. This URI (http://www.example.com) is ignored -->
                <to uri="http://www.example.com" />
                <!-- <to uri="file://{{librarycloud.files.basepath}}/ingest-via-file-url?fileName=via-file-${id}" /> -->
                <to uri="direct:via-ingest-file"/>
            </split>
        </route>
        <route id="via-ingest-file" errorHandlerRef="eh">
            <from uri="direct:via-ingest-file"/>
            <doTry>
                <split streaming="true">
                    <tokenize token="viaRecord" xml="true"/>
                    <setHeader headerName="outerSplitIndex">
                        <simple>${header.CamelSplitIndex}</simple>
                    </setHeader>
    
                    <split streaming="true" parallelProcessing="true">
                        <method bean="viaRawSplitter" method="split"/>
                        <aggregate strategyRef="modsVIARawAggregator" completionSize="100" completionInterval="10000">
                            <correlationExpression>
                                <simple>all</simple>
                            </correlationExpression>
                            <completionPredicate>
                                <simple>${header.messageLength} > 150000</simple>
                            </completionPredicate>  
                            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient" />
                            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-start?fileName=${header.outerSplitIndex}-${header.CamelSplitIndex}" /> -->
                        </aggregate>
                    </split>
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
        </route>

        <!-- Route messages to the correct part of the enrich pipeline -->
        <route id="enrich-start" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}//enrich-start" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>            
            <choice>
                <when>
                    <xpath>//source = 'aleph'</xpath>
                    <to uri="seda:enrich-01?size=20&amp;blockWhenFull=true" />
                </when>
                <when>
                    <xpath>//source = 'ALEPH'</xpath>
                    <to uri="seda:enrich-01?size=20&amp;blockWhenFull=true" />
                </when>
                <otherwise>
                    <!-- Skip Holdings, Stackscore, and LCCSH steps for non-aleph records -->
                    <!--<to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-05?amazonSQSClient=#sqsClient" />-->
                    <to uri="seda:enrich-05?size=20&amp;blockWhenFull=true" />
                    <!--<to uri="file://{{librarycloud.files.basepath}}//enrich-05" />-->
                </otherwise>
            </choice>
        </route>

        <route id="addholdingstomods-seda" errorHandlerRef="eh">
            <from uri="direct:enrich-01"/>
            <to uri="seda:enrich-01?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addholdingstomods" errorHandlerRef="eh">
            <from uri="seda:enrich-01?concurrentConsumers=1"/>
            <process ref="holdingsProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-02?amazonSQSClient=#sqsClient" />
            <!--<to uri="file://{{librarycloud.files.basepath}}/enrich-02" />-->
        </route>

        <route id="addstackscoretomods-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}/enrich-02" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-02?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:enrich-02?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addstackscoretomods" errorHandlerRef="eh">
            <from uri="seda:enrich-02?concurrentConsumers=1"/>
            <process ref="stackscoreProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-03?amazonSQSClient=#sqsClient" />
            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-03" /> -->
        </route>

        <route id="addslcctomods-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-03?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:enrich-03?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addslcctomods" errorHandlerRef="eh">
            <from uri="seda:enrich-03?concurrentConsumers=1"/>
            <process ref="lccProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-04?amazonSQSClient=#sqsClient" />
            <!--<to uri="file://{{librarycloud.files.basepath}}/enrich-04" />-->
            <!--<multicast>
                <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-04?amazonSQSClient=#sqsClient" />
                <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-05?amazonSQSClient=#sqsClient" />
            </multicast>-->
        </route>


        <route id="alephmodssplit-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-04?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:enrich-04?size=20&amp;blockWhenFull=true" />
        </route>

        <route id="alephmodssplit" errorHandlerRef="eh">
            <!--<from uri="file://{{librarycloud.files.basepath}}//enrich-04" />-->
            <from uri="seda:enrich-04?concurrentConsumers=5"/>
            <!--<from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-04?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10"/>-->
            <!--<marshal><custom ref="cloudbody"/></marshal>-->
            <doTry>
                <split streaming="true" parallelProcessing="true">
                    <method bean="modsMessageSplitter" method="split"/>
                    <!--<marshal><custom ref="cloudbody"/></marshal>-->
                    <aggregate strategyRef="modsRawAggregator" completionSize="100" completionInterval="10000">
                        <correlationExpression>
                            <simple>all</simple>
                        </correlationExpression>
                        <completionPredicate>
                            <simple>${header.messageLength} > 150000</simple>
                        </completionPredicate>
                        <marshal><custom ref="cloudbody"/></marshal>
                        <!--<to uri="seda:enrich-05?size=20&amp;blockWhenFull=true" />-->
                        <!--<to uri="file://{{librarycloud.files.basepath}}/enrich-05" />-->
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-05?amazonSQSClient=#sqsClient" />
                    </aggregate>
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <!--<to uri="file://{{librarycloud.files.basepath}}/dead-letter" />-->
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
        </route>

        <route id="adddrsextensionsstomods-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-05?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:enrich-05?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="adddrsextensionstomods" errorHandlerRef="eh">
            <!--<from uri="file://{{librarycloud.files.basepath}}//enrich-05" />-->
            <from uri="seda:enrich-05?concurrentConsumers=1"/>
            <doTry>
            <process ref="drsExtensionsProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
                <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-06?amazonSQSClient=#sqsClient" />
            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-02" /> -->
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                    <!--<to uri="file://{{librarycloud.files.basepath}}/dead-letter" />-->
                </doCatch>
            </doTry>
        </route>

        <route id="addcollectionstomods-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}//enrich-06" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-06?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>            
            <to uri="seda:enrich-06?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addcollectionstomods" errorHandlerRef="eh">
            <from uri="seda:enrich-06?concurrentConsumers=5"/>
            <process ref="collectionsProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-end?amazonSQSClient=#sqsClient" />
            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-end" /> -->
        </route>

        <!-- Publishing records to consumers -->
        <route id="publish-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}/enrich-end" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-end?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>            
            <to uri="seda:enrich-end?size=20&amp;blockWhenFull=true"/>
        </route>
        <route id="publish" errorHandlerRef="eh">
            <from uri="seda:enrich-end?concurrentConsumers=1" />
            <process ref="removeRestrictedProcessor"/>
            
            <multicast>
                <to uri="direct:publish-delta"/>
                <to uri="direct:publish-full"/>
            </multicast>
        </route>            

        <!-- Publish records that have changed -->
        <route id="publish-delta" errorHandlerRef="eh">
            <from uri="direct:publish-delta"/>
            <!-- Remove any items that haven't changed -->
            <!--<process ref="filterUnchangedProcessor"/>-->

            <!-- Don't try to publish empty messages -->
            <choice>
                <when>
                    <xpath>string-length(/lib_comm_message/payload/data) = 0</xpath>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-empty-message?amazonSQSClient=#sqsClient" />
                </when>
                <otherwise>
                    <marshal><custom ref="cloudbody"/></marshal>            
                    <multicast>
                        <!-- Publishing to Solr doesn't yet go through the SNS publishing, so put it on its own queue -->
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public-delta-solr?amazonSQSClient=#sqsClient" />
                        <to uri="aws-sns://harvard-librarycloud-publish-delta-{{librarycloud.sqs.environment}}?amazonSNSClient=#snsClient" />
                    </multicast>
                </otherwise>
            </choice>
        </route>

        <!-- Publish all records -->
        <route id="publish-full" errorHandlerRef="eh">
            <from uri="direct:publish-full"/>

            <!-- Don't try to publish empty messages -->
            <choice>
                <when>
                    <xpath>string-length(/lib_comm_message/payload/data) = 0</xpath>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-empty-message?amazonSQSClient=#sqsClient" />
                </when>
                <otherwise>
                    <marshal><custom ref="cloudbody"/></marshal>
                        <to uri="aws-sns://harvard-librarycloud-publish-full-{{librarycloud.sqs.environment}}?amazonSNSClient=#snsClient" />
                </otherwise>
            </choice>
        </route>

        <!-- SOLR consumer for loading into item API -->
        <route id="modstosolr-seda" errorHandlerRef="eh">
            <!--<from uri="file://{{librarycloud.files.basepath}}/publish-public-delta-solr" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public-delta-solr?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>            
            <to uri="seda:publish-public?size=20&amp;blockWhenFull=true"/>
        </route>
        <route id="modstosolr" errorHandlerRef="eh">
            <from uri="seda:publish-public?concurrentConsumers=1" />
            <process ref="solrLoadProcessor"/>
            <to uri="log:edu.harvard.libcomm.throughput?level=TRACE&amp;marker=solrLoadProcessor&amp;groupSize=10"/>
            <marshal><custom ref="cloudbody"/></marshal>            
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>
            <!-- <to uri="file://{{librarycloud.files.basepath}}/done" /> -->
        </route>

        <!-- SOLR consumer for loading into item API where the updates should be commited immediately -->
        <route id="modstosolr-immediate" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public-immediate?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <process ref="solrLoadProcessorImmediate"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>
        </route>

        <!-- DRS Extension Update handling - handles text file of urns, csv separated, line by line -->
        <route id="extension-update-ingest-split">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-extensions?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800" />
            <!-- Try block required for errors thrown out of the splitter -->
            <doTry>
                <split streaming="true" parallelProcessing="true">
                    <method bean="updateSplitter" method="split"/>
                    <marshal><custom ref="cloudbody"/></marshal>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-update-extensions?amazonSQSClient=#sqsClient" />
                    <!--<to uri="seda:update-extensions?size=20&amp;blockWhenFull=true" />-->
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
        </route>

        <route id="updateextensions-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-update-extensions?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:update-extensions?size=20&amp;blockWhenFull=true"/>
        </route>

        <route id="updateextensions" errorHandlerRef="eh">
            <!--<from uri="aws-sqs://{{librarycloud.sqs.environment}}-update-extensions?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />-->
            <from uri="seda:update-extensions?concurrentConsumers=500"/>
            <process ref="drsExtensionsNewUpdateProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-drsextensionstosolr?amazonSQSClient=#sqsClient"/>
        </route>

        <route id="drsextensionstosolr-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-drsextensionstosolr?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <unmarshal><custom ref="cloudbody"/></unmarshal>
            <to uri="seda:drsextensionstosolr?size=20&amp;blockWhenFull=true"/>
        </route>

        <route id="drsextensionstotosolr" errorHandlerRef="eh">
            <from uri="seda:drsextensionstosolr?concurrentConsumers=4" />
            <!--<from uri="aws-sqs://{{librarycloud.sqs.environment}}-split-holdings?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />-->
            <!--<from uri="file://{{librarycloud.files.basepath}}/split-holdings"/>-->
            <process ref="solrDrsExtensionsProcessor"/>
            <marshal><custom ref="cloudbody"/></marshal>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>
            <!--<to uri="file://{{librarycloud.files.basepath}}/done"/>-->
        </route>



        <!-- Delete handling -->
         <route id="aleph-delete-ingest-split">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-delete-aleph?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800" />
            <!-- Try block required for errors thrown out of the splitter -->
            <doTry>
                <split streaming="true">
                    <method bean="deleteSplitter" method="split"/>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-delete-public?amazonSQSClient=#sqsClient" />
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>                
            </doTry>
         </route>
        <route id="deletesolr" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-delete-public?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <process ref="solrDeleteProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>          
        </route>

        <!-- Collection update handling -->
        <route id="updatecollections-seda" errorHandlerRef="eh">
        <!--<from uri="file://{{librarycloud.files.basepath}}/updateCollection-input" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-update-public?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:update-public?size=20&amp;blockWhenFull=true" />
        </route>

        <route id="updateCollection" errorHandlerRef="eh">
            <from uri="seda:update-public?concurrentConsumers=1"/>
            <process ref="collectionUpdateProcessor"/>
            <choice>
                <when>
                    <xpath>string-length(/lib_comm_message/payload/data) = 0</xpath>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-empty-message?amazonSQSClient=#sqsClient" />
                </when>
                <otherwise>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public-immediate?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
                </otherwise>
            </choice>
        </route>

    </camelContext>
</beans>
